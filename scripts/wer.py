
import jiwer


def read_trans(file:str)->str:

    file1 = open(file,mode='r')
    lines = file1.readlines()
    outstr = ""
    for line in lines:
        line = line.strip()
        if line == "":
            pass
        else:
            outstr+=(line+' ')
    return outstr.strip()
##30s  84.97%
##20s  88.04%
##15s  86.55%

if __name__ == "__main__":
    recfile = "results/part1_20__rec.txt"
    labfile = "test_set/part1.txt"

    #rec_str = read_trans(recfile)
    #lab_str = read_trans(labfile)

    whisper_big = "learning a more general world model that can really just represent arbitrary things. In this case, what we do is we have a neural network that can be conditioned on the past or other things to predict the future. And I mean, obviously everyone has wanted to work on this forever. And I think with the recent rise in generative models, like, you know, transformers, diffusion, etc we finally have a shot at it what you're seeing here is purely generated video sequences given the past videos the network predicts some sample from the future hopefully the most likely sample and you can see that it is being predicted not just for one camera but it predicts the all the eight cameras around the car uh jointly and see how you know the car colors are consistent across the cameras the motion of objects is consistent in 3d even though we have not explicitly asked it to do anything in 3d or not baked in any 3d priors this is just the network understanding depth and motion on its own without us informing it of so and since this is all just predicting future you know rgb values the ontology is quite general you can like throw any video clip from driving or you know from YouTube or from your own phone, anything can be used to train this general dynamics model of the world. Additionally, it can also be action conditioned, being sure a few examples, but here on the left side, car is driving the lane and we're asking it to, okay, just keep in this lane and keep driving. And then, you know, like I said earlier, the car is able to, or the model is able to predict all of the geometry flow by uh very nicely and understands 3d on the right here we're asking to change lanes to the right side uh maybe we'll go back and play it again so on the left it's just going straight and we ask it to go straight the model goes straight and then on the right side we ask it to make a lane change and it makes a lane change and the past context is the same for both of these uh outputs so given the same past and when we ask it for different futures that model is able to produce or imagine different futures. This is super powerful because now you have essentially a neural network simulator that can simulate different futures based on different actions. And unlike a traditional game simulator, this is way more powerful because it can represent things that are very hard to describe in an explicit system. I'll show you a few more examples, but then it is super powerful and also the motion, the intention, and then the natural behavior of other objects such as vehicles is very hard to represent explicitly, but in this world is very easy to represent. It doesn't have to stop with just RGB, you can obviously do this kind of future prediction task, not just an RGB but also in optic segmentation. Or you can extend it to also 3D spaces where you can imagine future 3D scenes entirely based on just the past and then your action prompting or even without prompting, you can predict different futures. This is personally, you know, I'm amazed by how well this works. And it's a very exciting future that we are working on here. Yeah, here's some examples where I think, you know, something like this is going to be needed. to represent um like what's happening in the scene like there's a lot of smoke coming in one of the uh pictures like there's paper flying everywhere you know that's gonna be tough for you know even occupancy where okay you have paper flying everywhere this occupancy is occupancy flow but then how do you know it's paper what do you know the material properties of it um there's like smoke obviously you can drive through it but you know it is occupying space and light does not transmit through You know, there's a lot of nuances to driving and we have to really solve all of these problems to build a general driving stack that can drive anywhere in the world and be human-like, be fast, efficient at all speeds, yet very safe. And I think, you know, we're working on the right recipe for building this. And obviously, training all these models takes a ton of compute. that's why Tesla is aiming to become a world leader in compute. Dojo is our training hardware that we have custom built at Tesla that is starting production next month essentially and with that we think we are on the way to become one of the top compute platforms in the entire world and we also think that in order to train these foundational models for vision we need a lot of compute and not just train compute for training this one model but compute to try a lot of different experiments to see which models actually work well and that's why it's super exciting for us to you know. be in the spot where computer is going to be abundant and it's going to be modeled by ideas of engineers and practices, this is not just being built for the car, but also for the robot. We already have the occupancy network, for example, and other few other networks. all shared between the car and the robot"

    realtime_asr_ori = "I'm learning a more general world model that can really just represent arbitrary things. So in this case, what we do is we have a neural network that can be used to create a neural network. be conditioned on the past or other things to predict the future. And, uh, I mean, obviously everyone has wanted to. to work on this forever. And I think with the recent rise in generative models like, you know, transformational, commerce, diffusion etc. We finally have a shot at it. What you are seeing here is purely generated video sequences given the past video the network predicts some samples from the future, hopefully the most likely sample. And you can see. that it is being predicted not just for one camera but it predicts all the eight cameras around the car. jointly and see how the colors are consistent across the cameras, the motion of objects is consistent in 3D, even though we have not explicitly asked it to do anything in 3D or in 3D. not there and baked in any 3D priors. This is just the network understanding depth and motion and so on without us informing it off. So and since this is all just predicting. future RGB values. The ontology is quite general. You can throw any video clip from driving or from YouTube or from your own phone anything can be used to train this general dynamics model of the world. It can also be action conditioned. I'm making sure a few examples. So here on the left side, The car is driving the lane and we're asking you to just keep in this lane and keep driving. And then, like I said earlier, the model is able to predict all of the geometry flow by very nicely and understands 3D. On the right here, we're asking to change lanes to the right side. Maybe we'll go back and play it again. So on the left is it going straight. We ask it to go straight, the model goes straight. Then on the right side, we ask it to make a lane change and it makes a lane change. and the past context is the same for both of these outputs. So give them a call. the same past and when we ask it for different futures, then model is able to produce or like imagine different futures. This is super powerful because you know now you have essentially a neural network simulator that can simulate different futures based on different actions. a traditional game simulator, this is way more powerful because it can represent things that are very hard to describe in an explicitly system. a few more examples, but then it is super powerful and also the motion the intention and then the natural behavior of other objects such as vehicles is very hard to represent explicitly, but in this world is very easy to represent. It doesn't have to stop with just RGB. You can also just go back to the original version. obviously do this kind of future prediction task, not just not GB, but also in optic segmentation or reconnection it to also 3D spaces where you can imagine future 3D scenes entirely based on just the past and then your action prompting or even without prompting you can predict different futures. This is, personally, I'm amazed by how well this works. It's a very exciting future that we are working on here. Yeah, here's some examples, but I think something like this is going to be needed to represent. what's happening in the scene. Like there's a lot of smoke coming in all of the pictures, like there's paper flying everywhere. that's going to be tough for even occupancy where you have paper flying everywhere. the occupancy flow, but then how do you know it's paper, what do you know the material properties of it? There's like smoke, obviously you can drive through it, but you know it is occupying space and light does not transmit through. There's a lot of nuances to driving and we have to really solve all of these problems to build a driving stack that can drive anywhere in the world and be human-like, might be fast, efficient. all speeds yet very safe and I think you know we're working on the right recipe for building this. Now, obviously, training all these models takes a ton of compute and that's why Tesla is aiming to become a world leader in computers. Dojo is our training hardware that we have custom built at Tesla that is starting production and next month essentially. And with that, we think, We are on the way to become one of the top computer platforms in the entire world and we also think that not to train these foundation models for vision, we need a lot of compute and not just train compute for training this one model, but compute to try a lot of different experiments to see which models actually work well. And that's why it's super exciting. for us to be in the spot where computer is going to be abundant and it's just going to be bonded by IDS. of engineers. not just being built for the car but also for the robot. We already have the occupancy network, for example. and other few of the networks all shared between the car and the robot"

    realtime_asr_new = "learning a more general world model that can really just represent arbitrary things. case, what we do is we have a neural network that can be conditioned on the past or other things to predict the future. And I mean, obviously, everyone has wanted to work on this forever. And I think with the recent rise in generative models like transomers, diffusion, et cetera. We finally have a shot at it. What you're seeing here is purely generated video sequences, given the past videos, the network predicts. You can see that it is being predicted not just for one camera, but it predicts all the eight cameras around the car jointly. And you can see how the car colors are consistent across the cameras. The motion of objects is consistent in 3D, even though we have not explicitly asked it to do anything in 3D or not to be baked in any 3D priors. This is just the network understanding depth and motion on its own without us informing it off so. just predicting future RGB values. The ontology is quite general. You can throw any video clip from driving or from YouTube or from your own phone. Anything can be used to train this general dynamics model of the world. it can also be action conditioned. I'm making sure a few examples. So here on the left side, And we're asking it to, okay, just keep in this lane and keep driving. And then, like I said earlier, the model is able to predict all of the geometry flow by very nicely and understands 3D. the right here, we're asking to change lanes to the right side. Maybe we'll go back and play it again. So on the left, it's just going straight. straight. And on the right side, we ask it to make a lane change and make a lane change. And the past context is the same for both of these outputs. So given the same past, and when we ask it for different futures, then model is able to produce or imagine different futures. This is super powerful because now you have essentially a neural netlock simulator that can simulate different futures based on different actions. And unlike a traditional game simulator, you can't do anything about it. This is very powerful because it can represent... that are very hard to describe in an explicit system. I'll show a few more examples, but then it is super powerful. And also the motion, the. intention and then the natural behavior of other objects such as vehicles is very hard to represent explicitly, but in this world it's very easy to represent. top with just RGB. You can obviously do this kind of future prediction task, not just RGB, but also in product segmentation, or you can actually do also 3D spaces, where you can imagine future 3D. seen centrally based on just the past and then your action prompting or even without prompting, you can predict different futures. Personally, I'm amazed by how well this works. And it's a very exciting future that we are working on here. Yeah, here's some examples, but I think something like this is going to be needed to represent. like there's a lot of smoke coming in all of the pictures, like there's paper flying everywhere, you know, that's going to be tough for, you know, even occupancy where, okay, you have paper flying everywhere, there's occupancy, there's occupancy flow, but then how do you know it's paper, what do you know the material properties of it? but it is occupying space and light does not transmit through. There's a lot of nuances to driving and we have to really solve all of these problems to build a general driving stack that can drive anywhere in the world and be human-like, might be fast, efficient at all speeds, yet very safe. And I think we are working on the right recipe for. building this. training all these models takes a ton of compute and that's why Tesla is aiming to become a world leader in compute. Dojo is our training hardware that we have custom built at Tesla, that is starting production next month essentially. And with that, we. top, some people are from the entire world. thing that not to train these foundation models for vision, we need a lot of compute and not just train compute for training this one model, but compute to try a lot of different experiments to see which models actually work well. And that's why it's super exciting for us to, you know, be in the spot where compute is going to be abundant and it's just going to be bonded by ideas of engineers. This is not just being built for the car, but also for the robot. We already have the occupancy network, for example, and other few other networks all shared between the car and the robot"
    
    realtime_asr_new1 = "Learning a more general world model that can really just represent arbitrary things. So in this case, what we do is we have a neural network that can be conditioned on the past or other things to predict the future. And I mean, obviously everyone has wanted to work on this forever. And I think with the recent. rise in generative models like you know, transomers, diffusion, etc. We finally have a shot at it. What you are seeing here is purely generated video sequences. Given the past videos, the network predicts some samples from the future, hopefully the most likely sample. You can see that it is being predicted not just for one camera, but it predicts all the eight cameras around the car jointly. See how the car colors are consistent across the cameras, the motion of objects is consistent in 3D. Even though we have not explicitly asked it to do anything in 3D or not they are baked in any 3D priors, this is just the network understanding depth and motion on its own without us informing it off so. And since this is all just predicting future RGB values, the ontology is quite general. You can throw any video clip from driving or from YouTube or from your own phone. Anything can be used to train this general dynamics model of the world. Additionally, it can also be action conditioned. I'm going to show a few examples. So here on the left side. The car is driving the lane and we're asking it to just keep in this lane and keep driving. And then, you know, like I said earlier, the car is able to, or the model is able to predict all of the geometry flow by very nicely and understands 3D. On the right here, we're asking to change lanes to the right side. Maybe we'll go back and play it again. So on the left is it going straight. Have you asked it to go straight? The model goes straight. Then on the right side, we ask it to make a lane change and it makes a lane change. The past context is the same for both of these outputs. So, given the same past and when we ask it for different futures, then model is able to produce or imagine different futures. This is super powerful because now you have essentially a neural network simulator that can simulate different futures based on different actions. Unlike a traditional game simulator. This is very powerful because I can represent things that are very hard to describe in an explicit system. I'll show a few more examples, but then it is super powerful and also the motion, the. intention and then the natural behavior of other objects such as vehicles is very hard to represent explicitly, but in this world is very easy to represent. It doesn't have to stop with just RGB. You can obviously do this kind of future prediction task, not just RGB, but also in product segmentation, or you can actually do also 3D spaces, where you can imagine future 3D scenes entirely based on just the past and then your action prompting, or even without prompting, you can predict different futures. This is, personally, I'm amazed by how well this works, and it's a very exciting future that we are working on here. Yeah, here's some examples, but I think, you know, something like this is going to be needed to represent, like what's happening in the scene like there's a lot of smoke coming in all of the pictures like this paper flying everywhere. You know, that's going to be tough for you know, even occupancy where okay you have paper flying everywhere. You know, you There's smoke, obviously you can drive through it, but it is occupying space and light does not transmit through. There's a lot of nuances to driving and we have to really solve all of these problems to build a general driving stack that can drive anywhere in the world, and be human like, may be fast, efficient at all speeds, yet very safe. I think we're working on the right recipe for building this. Obviously, training all these models takes a ton of compute, and that's why Tesla is aiming to become a world leader in compute. Dojo is our training hardware that we have custom built at Tesla. That is starting production next month, essentially. And with that, we think we are on the way to become one of the top compute platforms in the entire world. We also think that not to train these foundational models for vision, we need a lot of compute, and not just train compute for training this one model, but compute to try a lot of different experiments to see which models actually work well. And that's why it's super exciting for us to be in the spot where compute is going to be abundant and it's just going to be bonded by ideas of engineers. And part of this is not just being built for the car, but also for the robot. We already have the occupancy network, for example, and other few of the networks all shared between the car and the robot"

    realtime_asr_new1_beam8 = "Learning a more general world model that can really just represent arbitrary things. So in this case, what we do is we have a neural network that can be conditioned on the past or other things to predict the future. And I mean, obviously, everyone has wanted to work on this forever. And I think with the recent. rise in generative models like, you know, transomers, diffusion, etc., we finally have a shot at it. What you are seeing here is purely generated video sequences. Given the past videos, the network predicts some samples from the future, hopefully the most likely sample. And you can see that it is being predicted not just for one camera, but it predicts all the eight cameras around the car jointly. And you can see how the car colors are consistent across the cameras. The motion of objects is consistent in 3D. Even though we have not explicitly asked it to do anything in 3D or not, we aren't baked in any 3D priors. This is just the network understanding depth and motion on its own without us informing it of so. And since this is all just predicting future RGB values, the ontology is quite general. You can throw any video clip from driving or from YouTube or from your own phone, anything can be used to train this general dynamics model of the world. Additionally, it can also be action conditioned. I'm going to show a few examples. So here on the left side. The car is driving the lane and we're asking it to, okay, just keep in this lane and keep driving. And then, you know, like I said earlier, the car is able to or the model is able to predict all of the geometry flow by very nicely and understands 3D. On the right here, we're asking to change lanes to the right side. Maybe we'll go back and play it again. So on the left, it's just going straight. Have you asked it to go straight? The model goes straight. And then on the right side, we ask it to make a lane change and it makes a lane change. And the past context is the same for both of these outputs. So given the same past, and when we ask it for different futures, the model is able to produce or imagine different futures. This is super powerful because now you have essentially a neural network simulator that can simulate different futures based on different actions. And unlike a traditional game simulator. This is way more powerful because I can, you know, represent. Things that are very hard to describe in an explicit system. I'll show a few more examples, but then it is super powerful and also the motion, the. Intention and then the natural behavior of other objects such as vehicles is very hard to represent explicitly, but in this world is very easy to represent. It doesn't have to stop with just RGB. You can obviously do this kind of future prediction task, not just RGB, but also in product segmentation, or you can actually do also 3D spaces, where you can imagine future 3D scenes entirely based on just the past, and then your action prompting, or even without prompting, you can predict different futures. Personally, I'm amazed by how well this works, and it's a very exciting future that we are working on here. Yeah, here's some examples, but I think, you know, something like this is going to be needed to represent, like what's happening in the scene like there's a lot of smoke coming in all of the pictures like this paper flying everywhere, you know, that's going to be tough for you know, even occupancy where okay you have paper flying everywhere. There's occupancy there's occupancy flow but then how do you know it's paper what do you know the material properties of it. There's smoke, obviously you can drive through it, but it is occupying space and light does not transmit through. There's a lot of nuances to driving and we have to really solve all of these problems to build a general driving stack that can drive anywhere in the world and be human-like, might be fast, efficient at all speeds, yet very safe. I think we're working on the right recipe for building this. Obviously, training all these models takes a ton of compute, and that's why Tesla is aiming to become a world leader in compute. Dojo is our training hardware that we have custom built at Tesla. That is starting production next month, essentially, and with that, we think we are on the way to become one of the top compute platforms in the entire world, and we also think that to train these foundation models for vision, we need a lot of compute, and not just compute for training this one model, but compute to try a lot of different experiments to see which models actually work well, and that's why it's super exciting for us to be in this spot where compute is going to be abundant and it's just going to be bonded by ideas of engineers. I thought this was this is not just being built for the car, but also for the robot. We already have the occupancy network. For example, another few of the networks all shared between the car and the robot"

    wer = jiwer.wer(whisper_big,realtime_asr_ori)
    print("ori: ",1-wer)

    wer = jiwer.wer(whisper_big,realtime_asr_new)
    print("new: ",1-wer)

    wer = jiwer.wer(whisper_big,realtime_asr_new1)
    print("new1: ",1-wer)

    wer = jiwer.wer(whisper_big,realtime_asr_new1_beam8)
    print("new1_beam8: ",1-wer)

    